{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_emotions.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Case Folding ---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0    @tiffanylue i know  i was listenin to bad habi...\n",
      "1    layin n bed with a headache  ughhhh...waitin o...\n",
      "2                  funeral ceremony...gloomy friday...\n",
      "3                 wants to hang out with friends soon!\n",
      "4    @dannycastillo we want to trade with someone w...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# Mengunakan fungsi Series.str.lower() untuk mengubah hufur kapital menjadi huruf kecil pada Pandas\n",
    "df['content'] = df['content'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(df['content'].head())\n",
    "\n",
    "#save to csv\n",
    "df.to_csv('tweet_emotions_caseFolding.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Tokenizing ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [tiffanylue, know, was, listenin, to, bad, hab...\n",
      "1    [layin, bed, with, headache, ughhhhwaitin, on,...\n",
      "2                    [funeral, ceremonygloomy, friday]\n",
      "3          [wants, to, hang, out, with, friends, soon]\n",
      "4    [dannycastillo, we, want, to, trade, with, som...\n",
      "Name: tweet_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk # Library nltk\n",
    "import string # Library string\n",
    "# impor modul regular expression \n",
    "import re # Library regex \n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "df['content'] = df['content'].apply(remove_number)\n",
    "\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "df['content'] = df['content'].apply(remove_punctuation)\n",
    "\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "df['content'] = df['content'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "df['content'] = df['content'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "df['content'] = df['content'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['tweet_tokens'] = df['content'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(df['tweet_tokens'].head())\n",
    "#save to csv\n",
    "df.to_csv('tweet_emotions_tokenize.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Filtering (Stopwprd Removal) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering Result : \n",
      "\n",
      "0    [tiffanylue, know, listenin, bad, habit, earli...\n",
      "1           [layin, bed, headache, ughhhhwaitin, call]\n",
      "2                    [funeral, ceremonygloomy, friday]\n",
      "3                         [wants, hang, friends, soon]\n",
      "4    [dannycastillo, want, trade, someone, houston,...\n",
      "Name: tweet_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('english')\n",
    "\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"tweet_emotions.csv\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "df['tweet_tokens_WSW'] = df['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "print('Filtering Result : \\n') \n",
    "print(df['tweet_tokens_WSW'].head())\n",
    "\n",
    "#save to csv\n",
    "df.to_csv('tweet_emotions_Filtering.csv',index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Stemming ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "df = pd.read_csv('tweet_emotions_tokenize.csv')\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "# #change to string\n",
    "# df['tweet_tokens_WSW'] = df['tweet_tokens_WSW'].astype(str)\n",
    "\n",
    "\n",
    "sentences = df['tweet_tokens'].tolist()\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    words=word_tokenize(sentences[i])\n",
    "    #List comprehension\n",
    "    words=[stemmer.stem(word) for word in words]\n",
    "    sentences[i]=' '.join(words)\n",
    "   \n",
    "# print(sentences)\n",
    "\n",
    "#save to csv\n",
    "d = {'col1': df['sentiment'], 'col2': sentences}\n",
    "sentences = pd.DataFrame(d)\n",
    "sentences.to_csv('tweet_emotions_stemming.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Clustering--------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " wut\n",
      " ur\n",
      " itel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nico\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " thereallilza\n",
      " aubreyoday\n",
      " anastasiaesp\n",
      " tweepsgood\n",
      " americatalk\n",
      " wussup\n",
      " cha\n",
      "Cluster 1:\n",
      " mi\n",
      " im\n",
      " just\n",
      " day\n",
      " friend\n",
      " got\n",
      " wa\n",
      " love\n",
      " work\n",
      " today\n",
      "Cluster 2:\n",
      " day\n",
      " im\n",
      " work\n",
      " good\n",
      " just\n",
      " wa\n",
      " happi\n",
      " thi\n",
      " love\n",
      " like\n",
      "\n",
      "Prediction\n",
      "Positif\n"
     ]
    }
   ],
   "source": [
    "#Clustering data ke dalam beberapa kategori atau cluster yaitu komen positif, negatif, dan netral\n",
    "#K-Means Clustering\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#load data\n",
    "df = pd.read_csv('tweet_emotions_stemming.csv')\n",
    "\n",
    "#mengubah data menjadi array\n",
    "sentences = df['col2'].values\n",
    "\n",
    "#mengubah data menjadi vektor\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "#menggunakan K-Means Clustering\n",
    "true_k = 3\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i)\n",
    "    for ind in model.cluster_centers_.argsort()[:, ::-1][i, :10]:\n",
    "        print(' %s' % vectorizer.get_feature_names()[ind])\n",
    "\n",
    "\n",
    "print(\"\\nPrediction\")\n",
    "Y = vectorizer.transform([\"writing a new song\"])\n",
    "prediction = model.predict(Y)\n",
    "if prediction == 0:\n",
    "    print(\"Negatif\")\n",
    "elif prediction == 1:\n",
    "    print(\"Netral\")\n",
    "else:\n",
    "    print(\"Positif\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------Labeling-------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----Classification------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----Predict-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---Evaluasi-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89e0abca33cf4a51fed40e84035a9d07b9b47798c9ddd5755346d755ba15d037"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
