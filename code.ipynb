{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project UAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...\n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!\n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 40000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tweet_emotions.csv')\n",
    "display(df.head())\n",
    "\n",
    "jml_baris_asli = df.shape[0]\n",
    "print(f'Jumlah baris: {jml_baris_asli}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris: 39827\n",
      "Jumlah baris duplikasi 173\n"
     ]
    }
   ],
   "source": [
    "# Drop twit yang sama\n",
    "df.drop_duplicates(subset=['content'], inplace=True)\n",
    "\n",
    "# Cek jumlah data\n",
    "jml_baris_drop = df.shape[0]\n",
    "print(f'Jumlah baris: {jml_baris_drop}')\n",
    "print(f'Jumlah baris duplikasi {jml_baris_asli - jml_baris_drop}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Mention (@)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>Layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>Funeral ceremony   gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends SOON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>We want to trade with someone who has Houston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1956989514</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@sweeetnspicy hiii im on my ipod...i cant fall...</td>\n",
       "      <td>hiii im on my ipod   i cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1956989526</td>\n",
       "      <td>sadness</td>\n",
       "      <td>dont wanna work 11-830 tomorrow  but i get paid</td>\n",
       "      <td>dont wanna work 11 830 tomorrow  but i get paid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1956989560</td>\n",
       "      <td>sadness</td>\n",
       "      <td>feels sad coz i wasnt able to play with the gu...</td>\n",
       "      <td>feels sad coz i wasnt able to play with the gu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1956989561</td>\n",
       "      <td>neutral</td>\n",
       "      <td>PrinceCharming</td>\n",
       "      <td>PrinceCharming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1956989601</td>\n",
       "      <td>hate</td>\n",
       "      <td>@ cayogial i wanted to come to BZ this summer ...</td>\n",
       "      <td>cayogial i wanted to come to BZ this summer ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_id   sentiment                                            content  \\\n",
       "0   1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1   1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2   1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3   1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4   1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "..         ...         ...                                                ...   \n",
       "95  1956989514     sadness  @sweeetnspicy hiii im on my ipod...i cant fall...   \n",
       "96  1956989526     sadness    dont wanna work 11-830 tomorrow  but i get paid   \n",
       "97  1956989560     sadness  feels sad coz i wasnt able to play with the gu...   \n",
       "98  1956989561     neutral                                     PrinceCharming   \n",
       "99  1956989601        hate  @ cayogial i wanted to come to BZ this summer ...   \n",
       "\n",
       "                                        content_clean  \n",
       "0    i know  i was listenin to bad habit earlier a...  \n",
       "1   Layin n bed with a headache  ughhhh   waitin o...  \n",
       "2                 Funeral ceremony   gloomy friday     \n",
       "3                wants to hang out with friends SOON   \n",
       "4    We want to trade with someone who has Houston...  \n",
       "..                                                ...  \n",
       "95            hiii im on my ipod   i cant fall asleep  \n",
       "96    dont wanna work 11 830 tomorrow  but i get paid  \n",
       "97  feels sad coz i wasnt able to play with the gu...  \n",
       "98                                     PrinceCharming  \n",
       "99    cayogial i wanted to come to BZ this summer ...  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # python regex lib\n",
    "\n",
    "df = df.copy()\n",
    "\n",
    "# Membuat kolom baru untuk kebutuhan berbandingan\n",
    "df['content_clean'] = df['content']\n",
    "\n",
    "# Membuat fungsi lambda untuk membuat mention, url\n",
    "rm_rt_url = lambda x: re.sub('(@[A-Za-z0-9\\w]+) | (@\\w+:) | (\\w+:\\/\\/\\S+) | (www.\\S+)',' ',x)\n",
    "rm_punct = lambda x: re.sub('\\W', ' ', x)\n",
    "\n",
    "# Membuat fungsi untuk membuang protocol internet\n",
    "\n",
    "# Map filter\n",
    "df['content_clean'] = df.content_clean.map(rm_rt_url).map(rm_punct)\n",
    "df.head(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "      <td>re pinging why didn t you go to prom  bc my bf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "      <td>i should be sleep  but im not  thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "      <td>hmmm  is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "      <td>charlene my love  i miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "      <td>i m sorry  at least it s friday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "5  1956968477       worry  Re-pinging @ghostridah14: why didn't you go to...   \n",
       "6  1956968487     sadness  I should be sleep, but im not! thinking about ...   \n",
       "7  1956968636       worry               Hmmm. http://www.djhero.com/ is down   \n",
       "8  1956969035     sadness            @charviray Charlene my love. I miss you   \n",
       "9  1956969172     sadness         @kelcouch I'm sorry  at least it's Friday?   \n",
       "\n",
       "                                       content_clean  \n",
       "0   i know  i was listenin to bad habit earlier a...  \n",
       "1  layin n bed with a headache  ughhhh   waitin o...  \n",
       "2                funeral ceremony   gloomy friday     \n",
       "3               wants to hang out with friends soon   \n",
       "4   we want to trade with someone who has houston...  \n",
       "5  re pinging why didn t you go to prom  bc my bf...  \n",
       "6  i should be sleep  but im not  thinking about ...  \n",
       "7                                      hmmm  is down  \n",
       "8                       charlene my love  i miss you  \n",
       "9                   i m sorry  at least it s friday   "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content_clean'] = df.content_clean.str.lower()\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>i know  i was listenin to bad habit earlier a...</td>\n",
       "      <td>[i, know, i, was, listenin, to, bad, habit, ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>layin n bed with a headache  ughhhh   waitin o...</td>\n",
       "      <td>[layin, n, bed, with, a, headache, ughhhh, wai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>funeral ceremony   gloomy friday</td>\n",
       "      <td>[funeral, ceremony, gloomy, friday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>wants to hang out with friends soon</td>\n",
       "      <td>[wants, to, hang, out, with, friends, soon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>we want to trade with someone who has houston...</td>\n",
       "      <td>[we, want, to, trade, with, someone, who, has,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment                                            content  \\\n",
       "0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2  1956967696     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  1956967789  enthusiasm               wants to hang out with friends SOON!   \n",
       "4  1956968416     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0   i know  i was listenin to bad habit earlier a...   \n",
       "1  layin n bed with a headache  ughhhh   waitin o...   \n",
       "2                funeral ceremony   gloomy friday      \n",
       "3               wants to hang out with friends soon    \n",
       "4   we want to trade with someone who has houston...   \n",
       "\n",
       "                                       content_token  \n",
       "0  [i, know, i, was, listenin, to, bad, habit, ea...  \n",
       "1  [layin, n, bed, with, a, headache, ughhhh, wai...  \n",
       "2                [funeral, ceremony, gloomy, friday]  \n",
       "3        [wants, to, hang, out, with, friends, soon]  \n",
       "4  [we, want, to, trade, with, someone, who, has,...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "df_stem = df.copy()\n",
    "\n",
    "tweet_token = TweetTokenizer()\n",
    "df_stem['content_token'] = df_stem['content_clean'].apply(tweet_token.tokenize)\n",
    "\n",
    "df_stem.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\IQRIMA\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content_token'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-11e5728e7ab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_stopwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_filtering'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_stem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'content_token'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords_removal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mdf_stem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\IQRIMA\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\IQRIMA\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'content_token'"
     ]
    }
   ],
   "source": [
    "df_stem = df.copy()\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword english\n",
    "list_stopwords = stopwords.words('english')\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"tweet_emotions.csv\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "df['content_filtering'] = df_stem['content_token'].apply(stopwords_removal) \n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemming(text):\n",
    "    stem_text = [stemmer.stem(word) for word in text]\n",
    "    return stem_text\n",
    "\n",
    "df_stem['content_stem'] = df_stem['content_token'].apply(lambda x: stemming(x))\n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TextBlog Package\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Membuat fungsi untuk menghitung polarity\n",
    "def get_polarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "df_stem['polarity'] = df_stem['content_clean'].apply(get_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition(c):\n",
    "    if c>0:\n",
    "        return \"Positif\"\n",
    "    elif c==0:\n",
    "        return \"Neutral\"\n",
    "    else:\n",
    "        return 'Negatif'\n",
    "\n",
    "df_stem['sentiment_cluster'] = df_stem['polarity'].apply(condition)\n",
    "\n",
    "df_stem.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling sentiment_cluster and make new column with name labeling from sentiment_cluster\n",
    "df_stem['labeling'] = df_stem['sentiment_cluster'].map({'Positif': 1, 'Neutral': 0, 'Negatif': -1})\n",
    "df_stem.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek jumlah data Pastikan Sesuai\n",
    "print(df_stem['sentiment_cluster'].value_counts())\n",
    "print(df_stem['labeling'].value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat Clasification with naive bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_stem['content_clean'], df_stem['labeling'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "# Import Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "label = {1: 'Positif', 0: 'Neutral', -1: 'Negatif'}\n",
    "y_test = y_test.map(label)\n",
    "y_pred = pd.Series(y_pred).map(label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Prediction with new data\n",
    "new_data = ['I love you so much', 'I hate you so much', 'I am so happy', 'I am so sad', 'I am so angry', 'I am so bored']\n",
    "\n",
    "# new_data = input('Masukkan teks: ')\n",
    "# new_data = [new_data]\n",
    "\n",
    "# Vectorize\n",
    "new_data = tfidf.transform(new_data)\n",
    "\n",
    "# Predict\n",
    "new_pred = model.predict(new_data)\n",
    "\n",
    "# Evaluation\n",
    "new_pred = pd.Series(new_pred).map(label)\n",
    "print(new_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for evaluation\n",
    "from sklearn.metrics import classification_report,precision_score,recall_score,accuracy_score\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f'Accuracy\\t: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision\\t: {precision_score(y_test, y_pred, average=\"macro\")}')\n",
    "print(f'Recall\\t\\t: {recall_score(y_test, y_pred, average=\"macro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Confussion Metric\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Confusion Matrix' , confusion_matrix(y_test, y_pred), sep='\\n')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predict')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AUC Evaluation\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Predict Probability\n",
    "y_pred_proba = model.predict_proba(X_test)\n",
    "\n",
    "# Get AUC Score\n",
    "auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "\n",
    "# Get ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1], pos_label=1)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.plot(fpr, tpr, label='ROC Curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b825a9d789970fa172fb9486e492e4bfded4591d4fa1b7208bcb9d0f75f74558"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
